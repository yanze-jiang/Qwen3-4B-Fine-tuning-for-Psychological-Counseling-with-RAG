import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import re

# ================= é…ç½® =================
BASE_MODEL_PATH = "/root/.cache/modelscope/hub/models/Qwen/Qwen3-4B"
LORA_PATH = "/root/autodl-tmp/qwen-psy-trained"  # ä¿ç•™ä½†ä¸ä½¿ç”¨
BENCHMARK_PATH = "/root/autodl-tmp/PsychCounsel-Bench.json"
MAX_NEW_TOKENS = 50
# ========================================

def load_benchmark(path):
    """åŠ è½½ benchmark æ•°æ®"""
    print("ğŸ“¥ åŠ è½½ benchmark æ•°æ®...")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"å…± {len(data)} é“é¢˜")
    return data

def build_psychology_prompt(item):
    """ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆæç¤ºè¯"""
    question = item["question"]
    options = item["options"]

    prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œè¯·åŸºäºå¿ƒç†å­¦ä¸“ä¸šçŸ¥è¯†é€‰æ‹©æœ€åˆé€‚çš„ç­”æ¡ˆã€‚
é—®é¢˜ï¼š{}

é€‰é¡¹ï¼š
{}
è¯·åªè¾“å‡ºé€‰é¡¹å­—æ¯ï¼ˆA/B/C/D/Eï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚""".format(
        question,
        "\n".join([f"{k.upper()}. {options[k]}" for k in sorted(options.keys())])
    )
    return prompt

def build_simple_prompt(item):
    """ç®€å•ç›´æ¥æç¤ºè¯"""
    question = item["question"]
    options = item["options"]

    prompt = f"{question}\n\n"
    for k in sorted(options.keys()):
        prompt += f"{k.upper()}. {options[k]}\n"
    prompt += "\nè¯·é€‰æ‹©æ­£ç¡®ç­”æ¡ˆçš„å­—æ¯ï¼š"
    return prompt

def build_chat_format_prompt(item):
    """å¯¹è¯æ ¼å¼æç¤ºè¯"""
    question = item["question"]
    options = item["options"]

    user_content = f"{question}\n\né€‰é¡¹ï¼š\n"
    for k in sorted(options.keys()):
        user_content += f"{k.upper()}. {options[k]}\n"
    user_content += "\nè¯·åªè¾“å‡ºé€‰é¡¹å­—æ¯ï¼š"

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œè¯·æ ¹æ®å¿ƒç†å­¦çŸ¥è¯†é€‰æ‹©æœ€åˆé€‚çš„ç­”æ¡ˆã€‚"},
        {"role": "user", "content": user_content}
    ]
    return messages

def extract_answer_v2(text):
    """ç­”æ¡ˆæå–"""
    if not text:
        return None

    text = text.strip()

    prefixes = ["å›ç­”ï¼š", "ç­”æ¡ˆæ˜¯", "é€‰æ‹©", "æˆ‘è®¤ä¸ºæ˜¯", "é€‰é¡¹", "ç­”ï¼š", "answer:", "answer is"]
    for p in prefixes:
        if text.lower().startswith(p.lower()):
            text = text[len(p):].strip()

    patterns = [
        r'^([A-Ea-e])[).\s]*',
        r'ç­”æ¡ˆæ˜¯\s*([A-Ea-e])',
        r'é€‰æ‹©\s*([A-Ea-e])',
        r'\b([A-Ea-e])\b',
    ]

    for pat in patterns:
        m = re.search(pat, text, re.IGNORECASE)
        if m:
            return m.group(1).lower()

    if text and text[0].lower() in "abcde":
        return text[0].lower()

    return None

def load_finetuned_model():
    """åŠ è½½å¾®è°ƒå‰çš„åŸºç¡€æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½å¾®è°ƒå‰åŸºç¡€æ¨¡å‹...")

    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        use_cache=False
    )

    model.eval()
    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {model.device}")
    return model, tokenizer

def evaluate_with_prompt_type(model, tokenizer, benchmark, prompt_type):
    print(f"\nğŸš€ ä½¿ç”¨ '{prompt_type}' æç¤ºè¯è¿›è¡Œè¯„ä¼°...")
    correct = 0
    wrong_details = []

    for idx, item in enumerate(tqdm(benchmark, desc="è¯„æµ‹è¿›åº¦")):
        if prompt_type == "simple":
            prompt_text = build_simple_prompt(item)
        elif prompt_type == "chat":
            messages = build_chat_format_prompt(item)
            try:
                prompt_text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except:
                prompt_text = build_simple_prompt(item)
        else:
            prompt_text = build_psychology_prompt(item)

        inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                temperature=0.1,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        gen_text = tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        pred = extract_answer_v2(gen_text)
        gold = item["answer"].lower()

        if pred == gold:
            correct += 1
        else:
            wrong_details.append({
                "index": idx,
                "question": item["question"][:100],
                "true": gold,
                "pred": pred,
                "gen": gen_text[:200]
            })

        if idx % 100 == 0 and torch.cuda.is_available():
            torch.cuda.empty_cache()

    acc = correct / len(benchmark) * 100
    return acc, correct, len(benchmark), wrong_details

def main():
    print("=" * 60)
    print("ğŸ§  å¿ƒç†å’¨è¯¢æ¨¡å‹ Benchmark æµ‹è¯•ï¼ˆBase Modelï¼‰")
    print("=" * 60)

    benchmark = load_benchmark(BENCHMARK_PATH)
    model, tokenizer = load_finetuned_model()

    prompt_types = [
        ("psychology", "ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆæ ¼å¼"),
        ("simple", "ç®€å•ç›´æ¥æ ¼å¼"),
        ("chat", "å¯¹è¯æ ¼å¼")
    ]

    results = {}
    best_acc = 0
    best_prompt = ""

    for ptype, desc in prompt_types:
        acc, correct, total, wrong = evaluate_with_prompt_type(
            model, tokenizer, benchmark, ptype
        )

        results[ptype] = {
            "accuracy": acc,
            "correct": correct,
            "total": total,
            "description": desc
        }

        print(f"{desc} | å‡†ç¡®ç‡: {acc:.2f}% ({correct}/{total})")

        if acc > best_acc:
            best_acc = acc
            best_prompt = ptype

    print("\nğŸ¯ æœ€ä½³æç¤ºè¯:", results[best_prompt]["description"])
    print(f"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")

    with open("benchmark_test_results_base.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("ğŸ’¾ ç»“æœå·²ä¿å­˜ï¼šbenchmark_test_results_base.json")

if __name__ == "__main__":
    main()