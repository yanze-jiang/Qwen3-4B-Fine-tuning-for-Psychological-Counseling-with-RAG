# local_qwen_inference.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import sys
import os

def main():
    # âœ… æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥ä½¿ç”¨ HuggingFace æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ï¼‰
    model_path = "Qwen/Qwen3-4B"  # æˆ–æ”¹ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚ "../../model_origin/Qwen3-4B"
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“± è®¾å¤‡: {model.device}")
        print(f"ğŸ”¢ å‚æ•°é‡: {model.num_parameters():,}")
        print(f"ğŸ’¾ æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    def generate_response(question):
        messages = [{"role": "user", "content": question}]
        
        try:
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=3000,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        return tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
    
    print("\nğŸ”¹ æµ‹è¯•é—®é¢˜ï¼šè¿™æ®µæ—¶é—´æˆ‘ä¸å¼€å¿ƒ")
    print("-" * 50)
    print(generate_response("è¿™æ®µæ—¶é—´æˆ‘ä¸å¼€å¿ƒ"))
    print("-" * 50)

if __name__ == "__main__":
    main()