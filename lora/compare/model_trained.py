# local_qwen_inference.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

def main():
    # âœ… å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„ï¼ˆLoRA é€‚é…å™¨ï¼‰
    base_model_path = "Qwen/Qwen3-4B"  # åŸºåº§æ¨¡å‹è·¯å¾„
    lora_path = "../training/qwen-psy-trained"  # LoRA é€‚é…å™¨è·¯å¾„
    
    if not os.path.exists(lora_path):
        print(f"âŒ LoRA é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
        return
    
    print(f"ğŸ“ ä½¿ç”¨åŸºåº§æ¨¡å‹: {base_model_path}")
    print(f"ğŸ“ ä½¿ç”¨ LoRA é€‚é…å™¨: {lora_path}")
    print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # å…ˆåŠ è½½åŸºåº§æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # ç„¶ååŠ è½½ LoRA é€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, lora_path)
        
        model.eval()
        
        print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“± è®¾å¤‡: {model.device}")
        print(f"ğŸ”¢ å‚æ•°é‡: {model.num_parameters():,}")
        print(f"ğŸ’¾ æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    def generate_response(question: str):
        messages = [{"role": "user", "content": question}]
        
        # âœ… å…³é—­ thinkingï¼ˆéå¸¸é‡è¦ï¼‰
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            thinking=False
        )
        
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        return tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
    
    print("\nğŸ”¹ æµ‹è¯•é—®é¢˜ï¼šè¿™æ®µæ—¶é—´æˆ‘ä¸å¼€å¿ƒ")
    print("-" * 60)
    print(generate_response("è¿™æ®µæ—¶é—´æˆ‘ä¸å¼€å¿ƒ"))
    print("-" * 60)

if __name__ == "__main__":
    main()