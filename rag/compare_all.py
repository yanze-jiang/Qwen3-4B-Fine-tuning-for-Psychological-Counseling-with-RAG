import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rag.retrieval import PsyRetriever
import os

# --- è·¯å¾„é…ç½® ---
BASE_MODEL_PATH = "Qwen/Qwen3-4B"  # æˆ–æ”¹ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚ "../model_origin/Qwen3-4B"
LORA_PATH = "../lora/training/qwen-psy-trained"  # LoRA é€‚é…å™¨è·¯å¾„

class ComparisonExperiment:
    def __init__(self):
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¿ƒç†å­¦å¯¹æ¯”å®éªŒ...")
        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹ (ä½¿ç”¨ bfloat16 æé«˜ç²¾åº¦å¹¶èŠ‚çœæ˜¾å­˜)
        print("ğŸ“¦ åŠ è½½åŸºåº§æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH, 
            torch_dtype=torch.bfloat16, 
            device_map="auto", 
            trust_remote_code=True
        )
        
        # æŒ‚è½½å¾®è°ƒå‚æ•°
        print("ğŸ’‰ æŒ‚è½½ LoRA é€‚é…å™¨...")
        self.psy_model = PeftModel.from_pretrained(self.base_model, LORA_PATH)
        
        # åˆå§‹åŒ– RAG æ£€ç´¢å™¨
        print("ğŸ” åˆå§‹åŒ– RAG æ£€ç´¢å™¨...")
        self.retriever = PsyRetriever()

    def clean_output(self, text):
        """ç§»é™¤æ¨ç†è¿‡ç¨‹ï¼Œåªä¿ç•™æœ€ç»ˆå›å¤"""
        if "</think>" in text:
            return text.split("</think>")[-1].strip()
        return text.strip()

    def generate(self, prompt):
        """é€šç”¨ç”Ÿæˆå‡½æ•°ï¼Œé…ç½®äº†é•¿æ–‡æœ¬å‚æ•°"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.psy_model.device)
        with torch.no_grad():
            outputs = self.psy_model.generate(
                **inputs, 
                max_new_tokens=1500,      # æ·±åº¦é•¿å›å¤æ”¯æŒ
                temperature=0.8,          # ä¿æŒå’¨è¯¢å¸ˆçš„è¯­è¨€çµæ´»æ€§
                top_p=0.95,
                repetition_penalty=1.1,   # é€‚åº¦æƒ©ç½šé‡å¤
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        full_res = self.tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        return self.clean_output(full_res)

    def run_compare(self, user_input):
        print("\n" + "â”"*60)
        print(f"ğŸŒŸ ç”¨æˆ·æé—®: {user_input}")
        print("â”"*60)

        # 1. åŸå§‹ Qwen3 æ¨¡å¼
        with self.psy_model.disable_adapter():
            prompt_base = f"<|im_start|>system\nä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
            res_base = self.generate(prompt_base)
            print(f"ğŸŸ¢ [1. åŸå§‹ Qwen3]:\n{res_base}\n")

        # 2. å¾®è°ƒå Qwen-Psy æ¨¡å¼
        # æ­¤æ—¶ adapter ä¼šè‡ªåŠ¨æ¢å¤å¯ç”¨
        res_psy = self.generate(prompt_base)
        print(f"ğŸ”µ [2. å¾®è°ƒå Qwen-Psy]:\n{res_psy}\n")

        # 3. å¾®è°ƒ + RAG æ¨¡å¼
        context = self.retriever.get_relevant_context(user_input)
        
        # ä¼˜åŒ–åçš„ RAG ç³»ç»Ÿæç¤ºè¯ï¼šåŒºåˆ†äº‹å®æŸ¥è¯¢ä¸æƒ…æ„Ÿå®‰æŠš
        rag_system = (
            f"ä½ æ˜¯ä¸€ä½èµ„æ·±å¿ƒç†å­¦ä¸“å®¶ã€‚å‚è€ƒèµ„æ–™å¦‚ä¸‹ï¼š\n{context}\n"
            "è¦æ±‚ï¼š\n1. å¦‚æœç”¨æˆ·è¯¢é—®ä¸“ä¸šæ¦‚å¿µï¼Œè¯·ä¼˜å…ˆåŸºäºèµ„æ–™ç»™å‡ºå‡†ç¡®è¯¦å°½çš„å®šä¹‰ã€‚\n"
            "2. å¦‚æœç”¨æˆ·è¡¨è¾¾æƒ…ç»ªï¼Œè¯·åœ¨å‚è€ƒèµ„æ–™çš„åŸºç¡€ä¸Šï¼Œç”¨é•¿ç¯‡å¹…è¿›è¡Œæ·±åº¦å…±æƒ…å’Œæ¸©æš–å¼•å¯¼ã€‚"
        )
        
        prompt_rag = f"<|im_start|>system\n{rag_system}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
        res_rag = self.generate(prompt_rag)
        print(f"ğŸ”¥ [3. å¾®è°ƒ + RAG]:\n{res_rag}\n")

if __name__ == "__main__":
    exp = ComparisonExperiment()
    
    # å…¸å‹æµ‹è¯•ç”¨ä¾‹
    test_queries